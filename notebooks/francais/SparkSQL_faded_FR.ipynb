{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous l'avions fait avant avec l'API RDD, d'abord nous initializons un Spark Context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous pouvons passer le Spark Context, initialisé dans <code>sc</code> , à la fonction <code>SQLContext</code>. Il est une convention d'appeler la variable <code>spark</code>, mais vous verrez ainsi des exemples où le nom de cette variable est <code>sql</code> ou bien <code>SQLContext</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'API Spark SQL, contrairement à l'API RDD, contient une méthode pour lire des fichiers CSV directement. Nous allons l'utiliser pour charger notre jeu de données de l'exemple du musée présenté à la fin de l'atelier sur l'API RDD. Une autre façon de se servir de l'API SparkSQL pour charger des CSVs c'est avec la syntaxe <code>spark.read.format(\"csv\").load('MetObjects.csv')</code>. En lisant ceci vous vous demandez peut-être si l'API SparkSQL offre des méthodes pour lire d'autres formats directement... et la réponse est OUI! Nous ne verrons que le CSV en exemple aujourd'hui, mais vous pouvez trouver une liste complète de sources supportées par Spark ici: https://spark.apache.org/docs/latest/sql-data-sources.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data = spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La commande ci-dessus a chargé notre CSV dans une structure de données appelée DataFrame. Si vous connaissez R ou la bibliothèque Python Pandas, vous vous sentirez chez vous avec l'API SparkSQL! \n",
    "\n",
    "La méthode <code>head</code> nous montre la première ligne d'un DataFrame. Vous remarquerez le mot 'Row' sur l'output: un DataFrame n'est qu'un RDD où les elements sont des objets de la classe <code>Row</code>!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre manière de voir ce qu'il y a dans un DataFrame est la méthode <code>show</code>. Ceci vous montrera votre DataFrame d'une manière semblable à celle de l'output d'une commande SQL sur la console d'une Base de Données Relationels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une troisième option c'est d'utiliser la méthode <code>toPandas</code>. Tel que le nom le suggère, cette méthode apportera votre DataFrame au Driver et le convertira en un DataFrame Pandas. Vous remarquerez la méthode <code>limit</code>, appliquée juste avant <code>toPandas</code>: puisque nous voulons apporter nos données au Driver, nous devons nous assurer de ne pas apporter une quantité trop élévée de lignes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous nous tournons vers la manipulation de données avec l'API SparkSQL. Nous commençons avec l'API DataFrames. La syntaxe resemble le SQL, mais au lieu d'écrire des commandes, nous appellons des méthodes exactement comme nous l'avons fait avec l'API RDD. Par exemple, nous pouvons faire un 'SELECT' pour retourner une seule colonne d'un DataFrame où une condition basée sur une autre colonne est satisfaite. En suite, nous comptons le nombre de lignes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pareil comme nous l'avons vu avec l'API RDD, vous pouvez utiliser la méthode <code>take</code> pour limiter la quantité de lignes retournées au Driver et en suite examiner les resultats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aussi de manière semblable au SQL, vous pouvez utiliser la méthode <code>distinct</code> pour ne retourner que des lignes uniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode <code>groupBy</code> fonctionne comme la commande SQL \"GROUP BY\": nous l'utilisons pour faire des aggrégations de nos données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode <code>OrderBy</code> fonctionne comme la commande SQL \"ORDER BY\": vous remarquerez que sa position dans la chaîne de méthodes est la même que celle d'une commande ORDER BY dans une sequence de commandes SQL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous vous demandez peut-être pourquoi nous avons choisi un catalogue d'oeuvres d'arts d'un musée pour l'atelier. La ligne ci-dessus nous montre la raison: c'est un jeu de données très en désordre! Regardez comme aucun des outputs ci-dessus ne resemble pas à un vrai nom d'artiste. La ligne suivante nous donne de ce qui se passe: il y a trop de guillemets dans quelques unes de nos colonnes! Regardez la colonne 'Titre' pour mieux comprendre ce que nous voulons dire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous nous servirons de Spark pour régler ces problèmes sous peu. Pour l'instant, regardons quelques autres trucs que nous pouvons faire avec l'API DataFrames, ainsi que quelques autres défauts dans notre jeu de données... Ici nous utilisons la méthode <code>like</code>  pour récupérer des lignes où na nationalité de l'artiste est un URL. Vous remarquerez la resemblence avec la syntaxe de SQL dans l'argument de la méthode <code>like</code>: le symbole 'wildcard' pour dire à Spark de chercher 'http suivi par n'importe quoi' est le '%'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode <code>where</code> retourne des lignes où la condition dans l'argument est satisfaite. L'équivalent de la commande SQL 'NOT LIKE', vous devez nier le resultat de <code>like</code> avec le symbole \"~\" (tilda). Vous remarquerez que les outputs sont en désordre ici aussi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ligne suivante nous donne encore une idée de ce qui se passe: regardez la première colonne, \"Object Number\". On y voit la string 'white cedar\"'. Le guillemet solitaire à la fin nous indique que ceci est probablement la fin d'une ligne entre guillemets qui a été cassée en deux car elle contient un (\\n)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous verrons comment régler ces problèmes sous peu. Avant de le faire, nous verrons pourquoi l'API s'appelle SparkSQL. La méthode <code>registerTempTable</code> vous pouvez transformer votre DataFrame en un vrai Tableau et se servir de SQL pour l'explorer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez ajoutes des multiples Tableaux à votre SQLContext actif et les utiliser tel que vous l'auriez fait dans une vraie Base de Données et faire de JOINs, des UNION... pour voir les Tableaux disponibles dans un SQLContext actif, nous utilisons la méthode <code>tableNames</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons vu comment créer des Tableaux à l'aide de <code>registerTempTable</code>. Pour enlever des Tableaux d'une session, utilisez <code>dropTempTable</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que votre DataFrame soit transformé en Tableau ou non, vous pouvez lister ses colonnes à l'aide du champ <code>columns</code> de l'objet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et vous pouvez effacer des colonnes d'un DataFrame avec la méthode <code>drop</code>. Ajouter des colonnes n'est pas aussi facile et nous allons voir comment le faire plus tard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SI vous connaissez bien le SQL, vous devez vous sentir comme chez vous avec l'API SparkSQL. Cependant, vous vous demandez peut-être... comment faire pour assigner des types aux colonnes? En d'autres mots, comment puis-je travailler sur une Basée de Données qui n'a pas de schéma?\n",
    "\n",
    "Il se trouve que des Tableaux ainsi que des DataFrames ONT des schémas dans Spark aussi! Vous pouvez le voir à l'aide de l'attribut <code>schema</code>: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre cas, nous n'avons pas spécifié le schéma de notre DataFrame lorsque nous l'avons chargé à partir du fichier CSV, alors Spark a decidé de lire toutes les colonnes comme si elles ne contenaient que du texte.\n",
    "\n",
    "Nous pouvons toutefois imposer un schéma à nos DataFrames avant de charger des données, pareillement comme nous le ferions en créant un Tableau vide avec du DDL sur une Base de Données.\n",
    "\n",
    "Vous verrez ci-dessous un exemple de comment nous aurions pu définir un schéma pour nos données du musée. Les entités suivantes sont dignes de prendre en note: <code>StructType</code>, <code>StructField</code> et les différents types de données. SI nous faisons une analogie avec le SQL, <code>StructType</code> sert a définir le contenu des colonnes sur une ligne et  <code>StructField</code> répresente une ligne au complèt.\n",
    "\n",
    "L'ordre des StructFields dans un StructType doit être la même que celle du jeux de données que vous souhaitez importer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "museum_schema = StructType([StructField('Object Number',StringType()), \n",
    "                     StructField('Is Highlight',     BooleanType()), \n",
    "                     StructField('Is Timeline Work', BooleanType()), \n",
    "                     StructField('Is Public Domain', BooleanType()),\n",
    "                     StructField('Object ID',        IntegerType()),\n",
    "                     StructField('Gallery Number',   IntegerType()),\n",
    "                     StructField('Department',       StringType()),\n",
    "                     StructField('AccessionYear',    IntegerType()),\n",
    "                     StructField('ObjectName',       StringType()),\n",
    "                     StructField('Ttile ',           StringType()),\n",
    "                     StructField('Culture',          StringType()),\n",
    "                     StructField('Period',           StringType()),\n",
    "                     StructField('Dynasty',          StringType()),\n",
    "                     StructField('Reign',            StringType()),\n",
    "                     StructField('Portfolio',        StringType()),\n",
    "                     StructField('Constituent ID',   IntegerType()),\n",
    "                     StructField('Artist Role',      StringType()),\n",
    "                     StructField('Artist Prefix',    StringType()),\n",
    "                     StructField('Artist Display Name',StringType()),\n",
    "                     StructField('Artist Display Bio',StringType()),\n",
    "                     StructField('Artist Suffix',    StringType()),\n",
    "                     StructField('Artist Alpha Sort',StringType()),\n",
    "                     StructField('Artist Nationality',StringType()),\n",
    "                     StructField('Artist Begin Date',IntegerType()),\n",
    "                     StructField('Artist End Date',  IntegerType()),\n",
    "                     StructField('Artist Gender',    StringType()),\n",
    "                     StructField('Artist ULAN URL',  StringType()),\n",
    "                     StructField('Artist Wikidata URL',StringType()),\n",
    "                     StructField('Object Date',      DateType()),\n",
    "                     StructField('Object Begin Date',IntegerType()),\n",
    "                     StructField('Object End Date',  IntegerType()),\n",
    "                     StructField('Object Display Name',StringType()),\n",
    "                     StructField('Medium',           StringType()),\n",
    "                     StructField('Dimensions',       StringType()),\n",
    "                     StructField('Credit Line',      StringType()),\n",
    "                     StructField('Geography Type',   StringType()),\n",
    "                     StructField('City',             StringType()),\n",
    "                     StructField('State',            StringType()),\n",
    "                     StructField('County',           StringType()),\n",
    "                     StructField('Country',          StringType()),\n",
    "                     StructField('Region',           StringType()),\n",
    "                     StructField('Subregion',        StringType()),\n",
    "                     StructField('Locale',           StringType()),\n",
    "                     StructField('Locus',            StringType()),\n",
    "                     StructField('Excavation',       StringType()),\n",
    "                     StructField('River',            StringType()),\n",
    "                     StructField('Classification',   StringType()),\n",
    "                     StructField('Rights and Reproduction',StringType()),\n",
    "                     StructField('Link Resource',    StringType()),\n",
    "                     StructField('Object Wikidata URL',StringType()),\n",
    "                     StructField('Metadata Date',    DateType()),\n",
    "                     StructField('Repository',       StringType()),\n",
    "                     StructField('Tags',             StringType()),\n",
    "                     StructField('Tags AAT URL',     StringType()),\n",
    "                     StructField('Tags Wikidata URL',StringType())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous allons importer notre CSV et lui imposer le schéma ci-dessus. Pour le faire, nous ajouterons le paramètre 'schema' à notre appel à la méthode <code>csv</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data = \n",
    "\n",
    "museum_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous nos données ont été chargées comme des objets de type <code>None</code>, ce que vous pouvez voir comme étant un équivalent en Python du <code>NULL</code> du SQL. Nous pouvons vérifier que, en effet, **tous** nos données ont été chargées en 'NULL' à l'aide de la ligne suivante. La méthode <code>dropna</code>, utilisée sans aucun argument, filtre toutes les lignes avec des NULLs de notre DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qu'est-ce que vient de se passer? Nous savons que notre jeu de donées est en désordre et que probablement il y a plusieurs lignes qui ne respecteront pas notre schéma. L'approche de SparkSQL lors du chargement de données est par défaut de 'planter' lorsqu'il y a des inputs mal-formatés!\n",
    "\n",
    "Nous pouvons changer l'approche en ajoutant une option à notre CSV reader pour ignorer des inputs mal-formatés et garder seulement ceux qui sont bien formatés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data = \n",
    "\n",
    "museum_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dirait qu'il y a vraiment beaucoup de lignes qui ne respectent pas notre schéma...\n",
    "\n",
    "De manière générale, imposer des schémas avec SparkSQL est une bonne idée quand nous savons que notre source de données est **propre** pour la plupart. C'est à dire, le nombre de colonnes et leurs types respecteront le schéma que nous voulons leur imposer. Avoir un schéma bien-défini en SparkSQL vous permettra d'utiliser des fonctions built-in directement, sans avoir à se soucier de convertir le type de vos colonnes, ou en créer des nouvelles juste pour se servir d'une fonction en particulier. Un bon exemple de scénario où avoir un schéma c'est gagnant, c'est quand nous faisons face à des données temporels avec des Dates ou des Timestapms.\n",
    "\n",
    "Puisque nous savons que notre jeu de données est vraiment en désordre, il est mieux de ne pas imposer un schéma et de laisser Spark le décider à la place. Nous ajouterons donc l'option 'inferschema' pour dire à Spark que nous aimerions qu'il essaie de inferer le schéma à partir de nos données.  **Spoiler alert:** Spark n'est pas très doué pour inferer des schémas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data = \n",
    "museum_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mieux, mais nous sommes encore loin d'avoir toutes les 500k lignes de notre CSV original.\n",
    "\n",
    "Nous avions identifié deux problèmes avec nos données:\n",
    "\n",
    "1. Trop de guillemets dans quelques lignes.\n",
    "2. Des line-breaks à l'interieur d'un ligne entre guillemets.\n",
    "\n",
    "Essayons de régler ces problèmes maintenant, et voyons si cela veut dire que nous aurons plus de lignes. D'abord nous utilisons l'option 'multiline' pour dire à Spark qu'il y a des line-breaks **à l'interieur** des colonnes de notre CSV, et elles n'indiquent pas la fin d'une ligne!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data = \n",
    "museum_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaucoup mieux! Vérifions si ceci a réglé notre problème avec les URLs dans la colonne qui devrait contenir la nationalité des artistes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il paraît que aumoins il n'y a plus de URL dans cette colonne. Vérifions si les URLs sont dans la bonne colonne maintenant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data.where(museum_data['Tags AAT URL']=='http://vocab.getty.edu/page/aat/300266506|http://vocab.getty.edu/page/aat/300386951|http://vocab.getty.edu/page/aat/300132410').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bingo! Maintenant essayons de régler le problème des guillemets en trop. Nous le faisons en ajoutant l'option 'escape', ce qui indique que toute occurence d'un guillemet (\") dans une colonne doit être lu comme un simple symbole et non pas un vrai guillemet capable de changer la structure du CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data = \n",
    "museum_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ça nous a donné 10k lignes de plus, pas pire! Vérifions l'état d'une des lignes \"brisées\" que nous avions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le point crucial ici est que beacoup des problèmes de qualité que nous observons souvant sur des données peuvent être réparés lors que nous les chargeons dans Spark. Cela s'applique également à la plupart des autres formats que le CSV supportés par Spark! ALors, si vous voyez des problèmes comme ceux que nous avons réglé, un bon premier pas c'est d'utiliser les options de la méthode <code>read</code> pour les réparer. Pour plus d'infos sur ces options, regardez ici: https://spark.apache.org/docs/latest/sql-data-sources.html\n",
    "\n",
    "Il reste encore d'autres problèmes dans notre jeu de données. Vous êtes les bienvnus pour essayer de les trouver. Il devrait être possible de les réparer en utilisant les options du CSV reader.\n",
    "\n",
    "Si par contre vous n'arrivez pas à régler un problème à l'aide de la méthode <code>read</code>, ou si vous souhaitez transformer une colonne qui a été chargé incorrectement, ou bien si vous voulez ajouter une nouvelle colonne à votre DataFrame, il y a un outil très puissant à votre disposition... l'API RDD!\n",
    "\n",
    "Vous pouvez extraire un RDD d'un DataFrame en lisant l'attribut <code>rdd</code> comme ceci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_rdd = museum_data.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un DataFrame est, comme nous l'avons déjà vu, un RDD où les éléments sont des objets de la classe Row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des objets de cette classe ne sont pas très utiles hors du contexte de l'API SparkSQL, nous convertissons alors tous les éléments en objets Python du type <code>lists</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_rdd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappellez-vous maintenant que nous vous avons dit que ajouter des colonnes à un DataFrame n'est pas facile? Il y plusieurs façons de le faire, mais aucune n'est particulièrement évidente. Celle que nous recommendons, à cause de la flexibilité qu'elle permet au programmeur est d'utiliser l'API RDD. Comme exemple, nous ajouterons un '1' à chaque élément de notre RDD si la nationalité de l'artiste est \"Canadian\", un '0' si l'artiste n'est pas Canadien et un 'NA' si la nationalité est un NULL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_rdd = museum_rdd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant créons une liste Python contenant les noms originels des colonnes de notre DataFrame, et ajoutons un seul nouveau nom: \"Is Canadian\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous allons convertir l'RDD en DataFrame à nouveau, et nous passerons notre liste de noms de colonnes comme un argument pour dire à Spark de quoi doit avoir l'air notre nouveau DataFrame. Le deuxième argument dit à Spark quel pourcentage de nos données Spark doit utiliser pour essayer d'inferer le schéma. Si dontre jeu de données est trop gros, même 1% (0.01) pourra causer des délais importants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museum_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et ça conclut notre introduction à l'API SparkSQL. Cette API est extrement puissante et il y a beaucoup d'autres fonctionalités que nous n'avons pas couvert dans l'atelier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 - Dans quelle année le musée a ajouté le plus d'oeuvres d'artistes Canadiens au catalogue?\n",
    "\n",
    "C'est votre tour! Utilisez l'API de votre choix (DataFrame ou SQL) pour trouver l'année dans laquelle le plus grand nombre d'ouvres d'artistes Canadiens a été ajouté au catalogue du musée. HINT: Vous pouvez vous servir de la colonne \"Is Canadian\" que nous avons crée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 - C'est l'exercise 1... en plus gros!\n",
    "\n",
    "Maintenant vous devez trouver l'année où le musée a ajouté le plus grand nombre d'oeuvres d'artistes de chaque nationalité présente dans le catalogue. Utilisez l'API RDD si vous voulez un défi!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 - Nouveau Tableau\n",
    "\n",
    "Créez un nouveau tableau contenant seulement les oeuvres dont le titre contient soit le mot \"Banjo\" **ou** le mot \"Clock\". Ajoutez une nouvelle colonne appelée 'Banjo Clock' à ce tableau avec le mot 'oui' si le titre de l'oeuvre contient 'Banjo' et 'Clock' et le mot 'non' sinon. HINT: la syntaxe des opérateurs logiques que vous devez utiliser dans les méthodes de SparkSQL n'est pas la même que celle de Python!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
