{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the Spark Streaming API in action, we will use a simple wordCount example from the Spark documentation. To get this working, first open a terminal window, or a terminal tab on JupyterHub, and run the following command:\n",
    "\n",
    "<code>ncat -lk 9999</code>\n",
    "\n",
    "Now, anything you type into that window will be visible at port 9999. The code block below sets up a spark Streaming Context (convention is to name it <code>ssc</code>) and tells spark to listen for anything that pops up on port 9999.\n",
    "\n",
    "We will pick up what we'll type on the other window and will run our familiar WordCount steps to... count the words we just typed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "#Uncomment the next line to run the code block on jupyter. Keep it commented if copy-pasting into the pyspark shell\n",
    "#sc = pyspark.SparkContext()\n",
    "\n",
    "# This tells Spark Streaming to bacth-up the contents of a data stream and \"ingest\" them every 10 seconds.\n",
    "ssc = StreamingContext(sc,10)\n",
    "\n",
    "# Tell spark to listen on port 9999 of our localhost.\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "words = lines.flatMap(lambda line : line.split(\" \"))\n",
    "\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCount = pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "wordCount.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods available in the Spark Streaming API are very similar to the ones from the RDD API. The example below sets up a Spark Streaming Context and tells Spark to **poll** (or monitor) a specific directory in our filesystem. Whenever a new file gets moved into that directory, Spark will ingest its contents as Text, just like we did with the RDD API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "#sc = pyspark.SparkContext()\n",
    "ssc = StreamingContext(sc,10)\n",
    "\n",
    "records = ssc.textFileStream(\"/home/user74/scratch/\")\n",
    "\n",
    "rows = records.map(lambda line : line.split(\",\"))\n",
    "rows.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen before, the RDD API is great, but the SparkSQL API really makes manipulating data feel more familiar for those who already know SQL or use R or Pandas regularly. You can leverage the SparkSQL API by making Spark Streaming populate a DataFrame as it reads the stream: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "spark = SQLContext(sc)\n",
    "\n",
    "my_dataframe = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells spark to listen on port 9999, just like we did before with SparkStreamingContext. The difference here is that we are telling Spark we want the contents of the stream to go into a DataFrame! Next, we inform Spark of what operations we wish to perform on our Stream. \n",
    "\n",
    "In this example, we will simply write the contents of the Stream to \"memory\" and call it \"new_dataframe\". We will also tell Spark to \"append\" new records to \"new_dataframe\" as they arrive. This is equivalent to registering a temp table like we did before, then populating it with the incoming contents of the stream!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x2b7856984090>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataframe.writeStream.format(\"memory\").queryName(\"new_dataframe\").outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that indeed we now have a registered table called \"new_dataframe\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_dataframe']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.tableNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can use SQL to query this table. Here, we assume the contents of the stream are comma separated values and we split them into columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Col1='this is a test', Col2=None, Col3=None),\n",
       " Row(Col1='test', Col2=None, Col3=None),\n",
       " Row(Col1='test', Col2=None, Col3=None),\n",
       " Row(Col1='test', Col2=None, Col3=None),\n",
       " Row(Col1='this', Col2=' is', Col3=' a')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT SPLIT(value,',')[0] AS Col1, SPLIT(value,',')[1] as Col2 FROM new_dataframe\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have demonstrated how Spark Streaming works by simulating a message broker that passes Text content over to Spark. This is a fairly common use case, but you will also often come across messages that are in a specific format, like JSON for example.\n",
    "\n",
    "The flow to set up a stream that handles files of a specific format is very similar to the examples above, it suffices to change the \"format\" option! See here for other supported formats: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - What is the Spread?\n",
    "\n",
    "In finance, the term **spread** is often used to refer to the difference between two metrics of interest. In the stock market in particular, the **Bid-Ask spread** is one of the many tools used to help inform decision making around a trader's positions in the market. The Bid-Ask spread is simply the difference between the price traders selling a security are asking, **the ask**, and the price traders buying that security are offering to pay, **the bid**. \n",
    "\n",
    "A prized piece of information that many firms pay good money to acquire is known as a **BBO** - the **Best Bid-Offer** datset. BBO datasets are simple tabular-formatted collections of data with 3 key columns: a timestamp, the best bid and the best offer for a given security at that exact timestamp. Stock brokers have a fiduciary duty to get the best bid for their clients selling a security and the best offer for their clients buying a security. Hence the usefulness of BBO data... but that is not all! If you have current BBO data, you can also keep an eye on the **spread** on a given security in real-time. Knowing this quantity in real-time can be used in many ways, the simplest of which is as a gauge of the **liquidity** of a security. In general, if the **spread** on a security is small, that suggests there is a hot market for that security (i.e. people are actively buying and selling the seurity). Conversely, if the spread is large, that suggests the market is not really interested in trading that security at that time.\n",
    "\n",
    "We will not show you how to create your own BBO dataset today, but we will use one to keep an eye on the spread of a certain stock. \n",
    "\n",
    "First, let's simulate a real-time feed from a BBO provider. To do this, run the following command on a terminal window:\n",
    "\n",
    "<code>stdbuf -oL cat data/14081.csv | ncat localhost 9999</code>\n",
    "\n",
    "Now, use the Structured Streaming approach we've seen before to read the BBO data into a DataFrame and get the **Spread** second-by-second. If you are looking for a challenge, try computing the **average spread** on a minute-by-minute basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Anomaly Detection\n",
    "\n",
    "Now let's look at another application of Spark: anomaly detection. In this problem, a fictional Utility Company started using Machine Learning to determine the price it charges its customers depending on the hour of the day. You believe this move on the part of the Utility Company causing you to pay way too much for electricity and have decided to put together a dossier exposing how their algorithm is out of whack. One way of exposing the weakness of their algorithm would be to catch anomalies in the price they are charging. The company themselves defines an anomaly as \"**a 2 standard deviation or larger increase over the average price for the same hour over the past two weeks.**\"\n",
    "\n",
    "Use the SparkSQL API to read the <code>utility.csv</code> and find instances of anomalous pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
